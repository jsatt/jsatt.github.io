<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jeremy Satterfield</title><link>https://jsatt.github.io/blog/</link><description>Coding, Making and Tulsa Life</description><category>development</category><language>en-us</language><lastBuildDate>Fri, 25 Feb 2022 18:18:59 -0600</lastBuildDate><atom:link href="https://jsatt.github.io/blog/feeds/rss/" rel="self" type="application/rss+xml"/><item><title>Resume</title><link>https://jsatt.github.io/resume/</link><pubDate>Sun, 30 Mar 2014 03:33:51 +0000</pubDate><guid>https://jsatt.github.io/resume/</guid><description>
&lt;div class="col-md-8 col-md-offset-2">
&lt;p>I am a full-stack web developer with a focus on best practices and testing to insure quality, readable and efficient code and reliable infrastructure. I enjoy working on civic and community driven open-source projects and volunteering for local hackathons and growing the local and regional developer community.&lt;/p>
&lt;h2 id="experience">Development Experience&lt;/h2>
&lt;h3 dir="ltr" id="docs-internal-guid-23803154-634b-5129-b7d2-abdb976f0230">Senior Software Developer - Halliburton (acquired Summit ESP in July 2017)&lt;/h3>
&lt;p dir="ltr">December 2014 - Present&lt;/p>
&lt;p dir="ltr" id="docs-internal-guid-5331bd05-3d11-0625-8a22-af23b7c8c34b">Working as technical lead, architecting and maintaining an in-house ticketing system using a Python/Django backend with a Django REST Framework based API, using Angular.js for the web frontend. Architected and lead the team on breaking our primary monolithic application into a microservice infrastructure. Lead the team in building a PySide/Qt based application which provided data entry and syncing with the ERP via the available API. Primary developer involved in DevOps managing SaltStack based configuration management for deploying several applications and architecting an experimental transition to a Docker, Kubernetes and Spinnaker test deployment pipeline. Involved in planning and design of new features including spending time with users to see how they interact with the system and working with product managers to help shape intuitive and efficient features and workflows&lt;/p>
&lt;p dir="ltr">&lt;em id="docs-internal-guid-5331bd05-3d18-d63c-c241-2001a381b792">Technologies used: Linux, Python, Django, MySQL, Javascript, CoffeeScript, Angular.js, Stylus/CSS, SaltStack, Docker, Kubernetes, Spinnaker, Amazon Web Services, Grunt, Gulp, StackStorm, Graphite/Statsd, Hubot&lt;/em>&lt;/p>
&lt;p>&lt;em>Projects contributed to: &lt;/em>&lt;a href="https://github.com/SummitESP/salt-states" target="_blank">Infrastructure Salt States&lt;/a>, &lt;a href="https://github.com/jsatt/python-catalog">Python Catalog&lt;/a>,&lt;a href="https://github.com/jsatt/python-catalog"> &lt;/a>&lt;a href="https://github.com/jsatt/django-db-email-backend">Django DB Email backend&lt;/a>, &lt;a href="https://github.com/jsatt/rest-client">Python REST Client&lt;/a>, &lt;a href="https://github.com/SummitESP/django-auth-ldap">Django LDAP Auth Backend&lt;/a>/&lt;a href="https://github.com/SummitESP/summit-auth-ldap">Summit LDAP Backend&lt;/a>&lt;/p>
&lt;h3>Senior Developer - ConsumerAffairs.com&lt;/h3>
&lt;p>December 2010 - December 2014&lt;/p>
&lt;p>Worked as a developer converting a code base from static PHP to a dynamic Python/Django site, and growing site traffic through improving site performance, A/B testing, and appropriate use of SEO. Worked as lead developer on a reputation management SaaS product, growing it from a new product to the company&amp;#39;s primary revenue source. Involved in planning of software stack and primary developer for continued work on a relatively small but efficient infrastructure capable of serving several million pageviews monthly, with resources to spare.&lt;/p>
&lt;p>&lt;em>Technologies used: Linux, Python, Django, MySQL, CoffeeScript/Javascript, Stylus/CSS, SaltStack, Brunch, CDN Storage, Celery&lt;/em>&lt;/p>
&lt;p>&lt;em>Projects contributed to&lt;/em>: &lt;a href="https://github.com/ConsumerAffairs/django-affect" target="_blank">Django Affect&lt;/a>, &lt;a href="https://github.com/ConsumerAffairs/django-urlographer" target="_blank">Django Urlographer&lt;/a>, &lt;a href="https://github.com/jsatt/django-test-utilities" target="_blank">Django Test Utilities&lt;/a>&lt;/p>
&lt;h3>Web Developer - OpenTundra&lt;/h3>
&lt;p>December 2008 - December 2010&lt;/p>
&lt;p>Worked as primary developer building the API, web and reporting interfaces for a vehicle and assert tracking system. Worked on kiosk software for managing and reporting fluid dispensing for stationary and mobile tanks and telemetry for vehicles.&lt;/p>
&lt;p>&lt;em>Technologies used: Linux, Coldfusion, MySQL, Python, Django, Javascript, CSS, Serial Communication, Hardware Interaction&lt;/em>&lt;/p>
&lt;h3>Analyst - AT&amp;amp;T Mobility&lt;/h3>
&lt;p>March 2007 - November 2008&lt;/p>
&lt;p>Working as primary developer and analyst for several intranet applications, including a custom reporting interface, a technical support CRM with dynamic report builder and correspondence CRM, as well as other in-house projects as needed. Built a custom authentication and permissions framework for these applications to provide granular role-based security.&lt;/p>
&lt;p>&lt;em>Technologies used: Coldfusion, MS-SQL Server, Javascript, CSS&lt;/em>&lt;/p>
&lt;h2 id="organizations">Organizations&lt;/h2>
&lt;h3>TulsaWebDevs - Volunteer&lt;/h3>
&lt;p>December 2010 - Present&lt;/p>
&lt;p>I have worked with the TulsaWebDevs participating in and helping to organize several events, such as civic hackathons, hackdays, monthly meetings and technology conferences to help the local developer community and the civic community as a whole.&lt;/p>
&lt;p>&lt;em>Projects contributed to: &lt;a href="https://github.com/tulsawebdevs/tulsa-road-issues-feed" target="_blank">TRIF&lt;/a>, &lt;a href="https://github.com/tulsawebdevs/tulsa-transit-google" target="_blank">Tulsa Transit GTFS&lt;/a>, &lt;a href="https://github.com/tulsawebdevs/django-boundaryservice" target="_blank">Django Boundary Service&lt;/a>&lt;/em>&lt;/p>
&lt;h3>Code for Tulsa - Delivery Lead&lt;/h3>
&lt;p>February 2014 - Present&lt;/p>
&lt;p>One of two Delivery Leads, volunteering for Code for Tulsa brigade of Code for America, responsible for having a working knowledge of all projects, understanding the needs of projects and connecting other brigade members to projects.&lt;/p>
&lt;h3>Civic Ninjas - Developer&lt;/h3>
&lt;p>December 2013 - July 2014&lt;/p>
&lt;p>Developing an application for citizens to analyze health demographics for specific cities and neighborhoods to better make healthy decisions for their families.&lt;/p>
&lt;p>&lt;em>Projects contributed to: &lt;a href="https://github.com/CivicNinjas/SitegeistHealth" target="_blank">HealthAround.me&lt;/a>&lt;/em>&lt;/p>
&lt;h2 id="skills">Skills&lt;/h2>
&lt;ul>
&lt;li>Python&lt;/li>
&lt;li>Django&lt;/li>
&lt;li>Linux Administration&lt;/li>
&lt;li>Amazon Web Services&lt;/li>
&lt;li>SaltStack&lt;/li>
&lt;li>HTML&lt;/li>
&lt;li>Nginx&lt;/li>
&lt;li>Javascript&lt;/li>
&lt;li>CoffeeScript&lt;/li>
&lt;li>CSS&lt;/li>
&lt;li>Stylus&lt;/li>
&lt;li>PostgreSQL&lt;/li>
&lt;li>MySQL&lt;/li>
&lt;li>GIS&lt;/li>
&lt;/ul>
&lt;h2 id="education">Education&lt;/h2>
&lt;h3>Tulsa Community College - Computer Science&lt;/h3>
&lt;p>2001 - 2003&lt;/p>
&lt;p>While attending, enrolled in courses in Computer science with a web design and development emphasis. Also took courses in accounting and business.&lt;/p>
&lt;p>&lt;em>Activities and Societies: Phi Theta Kappa Honor Society&lt;/em>&lt;/p>
&lt;h3>Tulsa Technology Center - Computer Science&lt;/h3>
&lt;p>1999 - 2002&lt;/p>
&lt;h2 id="links">Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/jsatt" target="_blank">GitHub&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.linkedin.com/in/jsatterfield">LinkedIn&lt;/a>&lt;/p>
&lt;/div>
&lt;div class="col-md-2">
&lt;ul class="nav" data-spy="affix" id="sidebar-nav">
&lt;li>&lt;a href="#experience">Experience&lt;/a>&lt;/li>
&lt;li>&lt;a href="#organizations">Organizations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#skills">Skills&lt;/a>&lt;/li>
&lt;li>&lt;a href="#education">Education&lt;/a>&lt;/li>
&lt;li>&lt;a href="#links">Links&lt;/a>&lt;/li>
&lt;/ul>
&lt;/div></description><dc:creator>Jeremy Satterfield</dc:creator></item><item><title>The Pains of MySQL Perfomance Under Docker</title><link>https://jsatt.github.io/blog/mysql-docker-performance/</link><pubDate>Mon, 15 Jan 2018 21:50:21 +0000</pubDate><guid>https://jsatt.github.io/blog/mysql-docker-performance/</guid><description>
&lt;p>Over the course of the last several days I&amp;#39;ve been plagued with a couple pretty massive performance issues running MySQL under Docker(-compose) in our developer environments. Everyone in our office is running either a Mac or some flavor of Linux. In both cases I spent a couple of days search for a solution, running into several roadblocks, and exhausting many Google search terms. Since I had such a difficult time finding the solution, here are the symptoms I was seeing and what finally worked in our case, maybe it&amp;#39;ll save someone else time.&lt;/p>
&lt;h2>Background&lt;/h2>
&lt;p>In both cases we were seeing incredibly long query times in certain areas of the site. The particular queries were those which we would expect to see performance issues exacerbated if they existed, large result sets with several joins and sorted. However, running the same queries against the same data on the same machines against MySQL installed natively on the host still ran these queries in fractions of a second, while the Docker instances took several minutes and spiked the CPU usage. This is an existing app with existing data, but only about 6GB on disk, so nothing big.&lt;/p>
&lt;p>We are using MySQL&amp;#39;s official images on Docker Hub. Using MySQL 5.6 to match what we currently have on our production databases. No customizations to speak of other than setting environment variables for user credentials and mounting named volumes to load and store the data.&lt;/p>
&lt;h2>The Issue with Mac&lt;/h2>
&lt;p>This was the fairly simple one to fix. After past experiences with similar CPU pegging performance issues with MySQL, I suspected this to be something to do with IO being limited somewhere. It took a bit of looking around, but I eventually ran across some forum posts talking about fsync flushing issues. I&amp;#39;ve run across enough similar issues in the past, it&amp;#39;s a wonder my mind doesn&amp;#39;t start there.&lt;/p>
&lt;p>So reading on, I came across &lt;a href="https://gist.github.com/kortina/67ad6e40e40d5199c3507cdad0c9a12c">this gist&lt;/a> which is a script to turn off fsync within the Hyperkit that Docker is running under Docker for Mac. This did the trick. I started up the docker containers and loaded page with the worst offending queries and it loaded in a matter of seconds.&lt;/p>
&lt;p>Since this was an issue with something as low level as fsync, you&amp;#39;d be right in thinking that there&amp;#39;s an issue with the driver Docker is using to access the disk. Further reading in one of the Github issues I found that they had apparently made changes&amp;nbsp;in 17.11+ to use`raw` format which no longer needed these fsync changes. Docker 17.12 landed in stable a couple days later and appears to have indeed fixed the issue.&lt;/p>
&lt;h2>The Issue with Linux&lt;/h2>
&lt;p>This proved to be a much tricker issue to fix and the solution turned out to be one I tried by chance and not anything I found existing on the web. Like Mac I was seeing CPU spike for complex queries and several minutes to return results. It turns out the Mac issue is wide spread enough that it&amp;#39;s nearly impossible to do a Google search that gives you Linux specific steps to try to narrow it down.&lt;/p>
&lt;p>Here is a short list of many of the steps that &lt;strong>did not work or showed minimal improvements&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>forcing Docker&amp;#39;s storage driver to something other than the default (I ended up on overlay2 because it&amp;#39;s recommended, but again there&amp;#39;s no noticeable difference)&lt;/li>
&lt;li>using a bind (named) volume for storing data files&lt;/li>
&lt;li>using a new mount volume from the host machine&lt;/li>
&lt;li>using a mount volume to the existing data files for my host machines existing MySQL data files, which worked great with a native instance&lt;/li>
&lt;li>upgrading to MySQL 5.7&lt;/li>
&lt;li>providing a custom settings file with tuned settings that worked for the native instance&lt;/li>
&lt;li>running a standalone Docker instance (read: not compose) with minimal parameters provided&lt;/li>
&lt;li>change how my root file system was mounted on boot to include &lt;code>barrier=0&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>At this point I was 3 days in and pretty frustrated. I decided to change tactics, and I set out to compare the performance of the Docker vs native instance in a better way. I started out by comparing the &lt;code>EXPLAIN&lt;/code> results for the same query in both environments, and instantly noticed a pretty big difference, the Docker instance wasn&amp;#39;t using a pretty important &lt;code>PRIMARY&lt;/code> index which the native instance is using. I dug a little deeper and no matter how the data was stored (bind v mount) if it was going through the Docker instance it wasn&amp;#39;t using the index. I verified that the index existing in all instances.&lt;/p>
&lt;p>Finally, for no reason that I can particularly pin down, I decided to run MySQL&amp;#39;s optimize script on all the tables. That fixed the issue. I have no idea why this fixed anything, but it has done the trick for all the Linux systems in our office. If you have an explanation I&amp;#39;d be happy to hear about it down in the comments.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Python Catalog Data Structures</title><link>https://jsatt.github.io/blog/python-catalog/</link><pubDate>Fri, 10 Nov 2017 15:07:49 +0000</pubDate><guid>https://jsatt.github.io/blog/python-catalog/</guid><description>
&lt;p>I recently realized I have several open-source projects I&amp;#39;m a maintainer or owner of which I&amp;#39;ve never really discussed on this site or on social media. Some of these projects have been pretty specific and tied to the other projects I was working on at the time, but several do have the potential for wider general use. I figured now was a good time to start talking about some of these projects and the problems they are intended to solve.&lt;/p>
&lt;h2>Python Catalogs&lt;/h2>
&lt;p>This is one of the more general use case projects I maintain. And while the examples and original impetus are for choices in Django, Catalogs could be used for just about any Python project that needs complex mappings. It&amp;#39;s a data structure that&amp;#39;s very simple to define and very flexible to use.&lt;/p>
&lt;p>You can install Catalog yourself via Pypi, &lt;code>pip install pycatalog&lt;/code>. As of the latest release it supports Python 2.7 and 3.3+. And you can always submit issues, feature requests and PRs over on &lt;a href="https://github.com/jsatt/python-catalog">Github&lt;/a>.&lt;/p>
&lt;h3>The Choice Definition Problem&lt;/h3>
&lt;p>One of the niceties that Django brings to the table for both models and forms, introduced in the some of the earliest tutorials, is allowing you to define choices. It&amp;#39;s a simple pattern that&amp;#39;s not limited to Django, Python or web development, but is weaved through much of Django&amp;#39;s out-of-the-box magic. It makes entry easier for users by providing their options to choose from and easier for your code to validate the input.&lt;/p>
&lt;p>It starts out easy enough: provide the database value and label for each choice item.&lt;/p>
&lt;pre class="language-python">
&lt;code>class SomeModel(models.Model):
status = models.IntegerField(choices=((1, &amp;#39;Open&amp;#39;), (2, &amp;#39;Closed&amp;#39;)), default=1)
&lt;/code>&lt;/pre>
&lt;p>Simple, but right off the bat this example exposes one of the first problems. Anytime you want to actually reference the value, &lt;code>default=1&lt;/code> in this case, you have to reference the value that&amp;#39;s going into the database. Hard coding these values can make it difficult to refactor as your project grows and also make code checking for these values crypt and unclear. Take, for example, the simplest of checks &lt;code>if sm.status == 1&lt;/code>.&lt;/p>
&lt;p>A better, more common pattern is to define these values as constants.&lt;/p>
&lt;pre class="language-python">
&lt;code>class SomeModel(models.Model):
OPEN = 1
CLOSED = 2
STATUS_CHOICES = (
(OPEN, &amp;#39;Open&amp;#39;),
(CLOSED, &amp;#39;Closed&amp;#39;),
)
status = models.IntegerField(choices=STATUS_CHOICES, default=OPEN)&lt;/code>&lt;/pre>
&lt;p>That&amp;#39;s much cleaner and the code references are clearer too, &lt;code>if sm.status == SomeModel.OPEN&lt;/code>.&lt;/p>
&lt;p>In most cases this is going to be exactly what your project needs and you&amp;#39;re done. Good work.&lt;/p>
&lt;h3>More Complex Cases&lt;/h3>
&lt;p>But what if data from this model syncs with a legacy remote API which has it&amp;#39;s own archaic idea of how these values should be presented? Not too hard, we can add a mapping for that. Well, actually, two mappings because we need to communicate both directions. No problem.&lt;/p>
&lt;pre class="language-python">
&lt;code>class SomeModel(models.Model):
OPEN = 1
CLOSED = 2
STATUS_CHOICES = (
(OPEN, &amp;#39;Open&amp;#39;),
(CLOSED, &amp;#39;Closed&amp;#39;),
)
API_STATUS_MAP = {
&amp;#39;op&amp;#39;: OPEN,
&amp;#39;cl&amp;#39;: CLOSED,
}
REVERSE_API_STATUS_MAP = {v: k for k, v in API_STATUS_MAP.items()}
status = models.IntegerField(choices=STATUS_CHOICES, default=OPEN)&lt;/code>&lt;/pre>
&lt;p>And sometimes we&amp;#39;ll want to go directly from the API value to the user-readable label. Maybe use a property or method for that? That might obscure the fact that the value is based on the status field, sometimes that&amp;#39;s good, but maybe not this time.&lt;/p>
&lt;p>Also, we want to use a specific template path for a re-used block that changes based on the choices, add another mapping.&lt;/p>
&lt;p>Oh, and maybe a different parser class to abstracts the different parsing details based on the choice, add another mapping.&lt;/p>
&lt;p>This is where things are starting to go haywire. More mappings, more properties and methods, more mess.&lt;/p>
&lt;h3>Enter Python Catalog&lt;/h3>
&lt;p>This is the snowball I was trying to stop when I started working on Catalog a couple years ago. We had a complex project that required a complicated web of mappings and lookup patterns. So I set out to find a data structure that could handle this case with a couple of goals in mind:&lt;/p>
&lt;ul>
&lt;li>Simple definition signature&lt;/li>
&lt;li>Developers could define their own attributes for the choices, to fit their use case&lt;/li>
&lt;li>Attributes could store values of arbitrary types&lt;/li>
&lt;li>Simple process for mapping items down to an iterable, containing just the attribute values needed at that point&lt;/li>
&lt;li>Easily lookup items by any attribute and access any attribute&lt;/li>
&lt;/ul>
&lt;p>Starting with the simple signature, Catalog works similar to &lt;a href="https://docs.python.org/3/library/enum.html">Python 3&amp;#39;s Enum&lt;/a>.&lt;/p>
&lt;pre class="language-python">
&lt;code>class STATUSES(Catalog):
open = 1
closed = 2
STATUSES.open.value # =&amp;gt;1
&lt;/code>&lt;/pre>
&lt;p>But that doesn&amp;#39;t even handle our simplest case of a database value and a label. We can do better. Let&amp;#39;s define more attributes.&lt;/p>
&lt;pre class="language-python">
&lt;code>class SomeModel(models.Model):
class STATUSES(Catalog):
_attrs = value, label, api_value
open = 1, &amp;#39;Open&amp;#39;, &amp;#39;op&amp;#39;
closed = 2, &amp;#39;Closed&amp;#39;, &amp;#39;cl&amp;#39;&lt;/code>&lt;/pre>
&lt;p>Oh, then there&amp;#39;s the template paths and parser classes.&lt;/p>
&lt;pre class="language-python">
&lt;code>class SomeModel(models.Model):
class STATUSES(Catalog):
_attrs = value, label, api_value, template, parser
open = 1, &amp;#39;Open&amp;#39;, &amp;#39;op&amp;#39;, &amp;#39;some_path/open.html&amp;#39;, OpenParser
closed = 2, &amp;#39;Closed&amp;#39;, &amp;#39;cl&amp;#39;, &amp;#39;other_path/closed.html&amp;#39;, ClosedParser&lt;/code>&lt;/pre>
&lt;p>Now let&amp;#39;s add those choices to the field. There&amp;#39;s an easy method for that.&lt;/p>
&lt;pre class="language-python">
&lt;code> status = models.IntegerField(choices=STATUSES._zip(&amp;#39;value&amp;#39;, &amp;#39;label&amp;#39;), default=STATUSES.open.value)&lt;/code>&lt;/pre>
&lt;p>Now I want to get the user-readable label, but I&amp;#39;ve got the remote api value, or parser based database value.&lt;/p>
&lt;pre class="language-python">
&lt;code>SomeModel.STATUSES(&amp;#39;op&amp;#39;, &amp;#39;api_value&amp;#39;).label # =&amp;gt; &amp;#39;Opened&amp;#39;
SomeModel.STATUSES(1, &amp;#39;value&amp;#39;).parser # =&amp;gt; OpenParser
&lt;/code>&lt;/pre>
&lt;h2>Conclusion&lt;/h2>
&lt;p>So there you have it. It&amp;#39;s by no means perfect, but after using it in production environments for over a year now, we still find it to be much cleaner and easier to use then some of the other patterns out there.&lt;/p>
&lt;p>While the access API is always just as simple, the definitions do get clunkier as you add more attributes to the items. It was never meant to handle dozens of attributes per item and you have bigger problems if that&amp;#39;s your use case, but since the attribute values for each item are really just presented as tuples, you can easily throw in parentheses and wrap across lines to help with readability.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Manage All the Languages Using Python Virtualenv</title><link>https://jsatt.github.io/blog/virtualenvs-for-all/</link><pubDate>Sat, 28 Oct 2017 01:17:32 +0000</pubDate><guid>https://jsatt.github.io/blog/virtualenvs-for-all/</guid><description>
&lt;p>A couple of years ago I was working with a couple other languages beside Python and began to get frustrated with their virtualenv equivalents. Not that they were doing anything wrong, just that they didn&amp;#39;t work the way I was used to with virtualenvs. On top of that, they didn&amp;#39;t work well together. I wanted to see if I could get something working that used the workflow I was used to and managed all the languages I was working with. So after a little poking around I found that many of the newer, &amp;quot;modern&amp;quot; languages had ways of running them from custom paths. I did a bit of work and before long I had a couple of these languages installed in virtualenvs and working side-by-side.&lt;/p>
&lt;p>Then, a couple days ago I ran across &lt;a href="https://twitter.com/freakboy3742/status/923316996489408512">this conversation on Twitter&lt;/a>.&lt;/p>
&lt;blockquote data-lang="en">
&lt;p dir="ltr" lang="en">Most common Python issue I see with students is they&amp;#39;ve installed Python three or four different ways &amp;amp; have all their paths confused. 1/&lt;/p>
&amp;mdash; Jake VanderPlas (@jakevdp) &lt;a href="https://twitter.com/jakevdp/status/922846245848150016?ref_src=twsrc%5Etfw">October 24, 2017&lt;/a>&lt;/blockquote>
&lt;blockquote data-lang="en">
&lt;p dir="ltr" lang="en">I need to write up how I manage hundreds of projects without this being a problem. It took years to figure it out but it works. &lt;a href="https://t.co/xM3fTs7e6u">https://t.co/xM3fTs7e6u&lt;/a>&lt;/p>
&amp;mdash; Jeff Triplett âœ¨ (@webology) &lt;a href="https://twitter.com/webology/status/923068721844948992?ref_src=twsrc%5Etfw">October 25, 2017&lt;/a>&lt;/blockquote>
&lt;blockquote data-lang="en">
&lt;p dir="ltr" lang="en">A new development I&amp;#39;m really excited about is not just virtualenv, but also putting any non-Python executables in the virtualenvwrapper bin dir so it&amp;#39;s associated with the project too.&lt;/p>
&amp;mdash; Rachel SKellyton ðŸ”® (@wholemilk) &lt;a href="https://twitter.com/wholemilk/status/923078806239264768?ref_src=twsrc%5Etfw">October 25, 2017&lt;/a>&lt;/blockquote>
&lt;blockquote data-lang="en">
&lt;p dir="ltr" lang="en">I have flirted with that for non-Python over the years. I liked it but lacked enough Node/Ruby knowledge to pull it off medium term.&lt;/p>
&amp;mdash; Jeff Triplett âœ¨ (@webology) &lt;a href="https://twitter.com/webology/status/923160645973012480?ref_src=twsrc%5Etfw">October 25, 2017&lt;/a>&lt;/blockquote>
&lt;blockquote data-lang="en">
&lt;p dir="ltr" lang="en">THIS. It&amp;rsquo;s always intrigued me that there isn&amp;rsquo;t a &amp;ldquo;virtualenv for *all*&amp;rdquo; - one env, virtualising Py, Ruby, C&amp;hellip; whatever.&lt;/p>
&amp;mdash; Russell Keith-Magee (@freakboy3742) &lt;a href="https://twitter.com/freakboy3742/status/923316996489408512?ref_src=twsrc%5Etfw">October 25, 2017&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>And I realized that I had solved this problem for myself a while back and never really mentioned to anyone except a couple coworkers from time to time, and I certainly never shared it on this site or social media. So I&amp;#39;m going to take this opportunity to share how I manage several different languages using virtualenv.&lt;/p>
&lt;p>One last note before we begin. I&amp;#39;m exclusively a Linux user and have been for many years. Most of these commands will work on Macs through the magic of Unix, but I have neither the knowledge or tools to troubleshoot Mac or Windows specific problems. Feel free to share solutions to problems you run into in the comments for others, but I will likely not be able to help too much for those OS&amp;#39;s. If you&amp;#39;re on Windows, please just use the &lt;a href="https://msdn.microsoft.com/en-us/commandline/wsl/about">Subsytem for Linux&lt;/a>.&lt;/p>
&lt;h2>Python&lt;/h2>
&lt;p>With virtualenv being part of the Python tool chain, you&amp;#39;re clearly going to need Python up and running. On Linux and Mac, your system probably already has it installed and ready to go. If your project works fine on this version or you&amp;#39;re only managing other languages, this version should be fine. If you want to use a specific version, check out&amp;nbsp;&lt;a href="https://github.com/pyenv/pyenv">pyenv&lt;/a> or &lt;a href="https://www.python.org/downloads/">downloading the source&lt;/a>, but they each have docs that can walk you through those processes.&lt;/p>
&lt;h3>Installing virtualenv&lt;/h3>
&lt;p>I recommend, and assume for the rest of this post, using virtualenvwrapper and virtualenvs stored at &lt;code>~/.virtualenv&lt;/code>. If you have other preferences, everything here should still work, but you will need to replace my references to &lt;code>~/.virtualenvs&lt;/code> with the path to your environment.&lt;/p>
&lt;p>Assuming you already have &lt;a href="https://pip.pypa.io/en/stable/installing/">pip installed&lt;/a>, with your Python version, you need to make sure that &lt;code>~/.local/bin&lt;/code> (or &lt;code>/home/&amp;lt;user&amp;gt;/.local/bin&lt;/code> or &lt;code>/Users/&amp;lt;user&amp;gt;/.local/bin&lt;/code>) is in your $PATH (&lt;code>echo $PATH&lt;/code>). If not, add the following to your &lt;code>~/.profile&lt;/code> (or .zprofile, config.fish, etc.)&lt;/p>
&lt;pre class="language-bash">
&lt;code>PATH=~/.local/bin:$PATH
&lt;/code>&lt;/pre>
&lt;p>If you needed to make this change, you should probably reboot your machine now to load that value into all your shell sessions.&lt;/p>
&lt;p>This is going to allow you to use the &lt;code>--user&lt;/code> flag for pip which installs the packages specified at a &amp;quot;global&amp;quot; level just for that user. This is useful for packages like &lt;code>virtualenv&lt;/code>, &lt;code>virtualenvwrapper&lt;/code>, &lt;code>ipdb&lt;/code>, etc which can be used to manage Python and your environments. This should NOT be used for packages used within your project.&lt;/p>
&lt;p>So with that set, install virtualenv:&lt;/p>
&lt;pre>
&lt;code>pip install --user virtualenv virtualenvwrapper&lt;/code>&lt;/pre>
&lt;p>Now you want need to edit your &lt;code>~/.profile&lt;/code> (or equivalent) and add the following and restart your shell.&lt;/p>
&lt;pre>
export WORKON_HOME=$HOME/.virtualenvs
source $HOME/.local/bin/virtualenvwrapper.sh
&lt;/pre>
&lt;h3>Create an Virtual Environment&lt;/h3>
&lt;p>&lt;code>virtualenvwrapper&lt;/code> make this easy. If you&amp;#39;re using your OS&amp;#39;s default Python, just use the following.&lt;/p>
&lt;pre>
&lt;code>mkvirtualenv myenv
&lt;/code>&lt;/pre>
&lt;p>If you want to use a different version you&amp;#39;ve installed elsewhere, use the &lt;code>-p&lt;/code> flag to specify it&amp;#39;s path.&lt;/p>
&lt;pre>
&lt;code>mkvirtualenv myenv -p /path/to/python&lt;/code>&lt;/pre>
&lt;p>From now on, to activate this environment again just use: &lt;code>workon myenv&lt;/code>.&lt;/p>
&lt;h3>Using virtualenvs&lt;/h3>
&lt;p>Since you&amp;#39;re opting to use virtualenvs for managing other languages, I&amp;#39;m going to assume you&amp;#39;re familiar with the day-to-day process of using them. If you&amp;#39;re not familiar, now is a good time to check out the &lt;a href="https://virtualenv.pypa.io/en/stable/">virtualenv&lt;/a> and &lt;a href="https://virtualenvwrapper.readthedocs.io/en/latest/">virtualenvwrapper&lt;/a> docs and Google for more info.&lt;/p>
&lt;h2>NodeJs&lt;/h2>
&lt;p>Node is incredibly easy to setup in virtualenvs, thanks largely to the fact that there is a Python package to handle it for you.&lt;/p>
&lt;pre>
&lt;code>pip install nodeenv&lt;/code>
&lt;/pre>
&lt;p>This package provides a tool that does everything for you. If you are using node in many projects, you might consider just installing nodeenv with the &lt;code>--user&lt;/code> flag as discussed above.&lt;/p>
&lt;p>Now install node using something like one of the following.&lt;/p>
&lt;pre>
&lt;code>nodeenv -p # install into active virtualenv
nodeenv .env/ # install into a virtualenv at a specific path&lt;/code>
nodeenv -p -n 8.8.1 # install a specific version of node
&lt;/pre>
&lt;p>That&amp;#39;s it. It&amp;#39;ll download and install the appropriate version, then update the activate script for your environment to provide all the environment variables Node needs to know it.&lt;/p>
&lt;p>Keep in mind that npm has the concept of &amp;quot;global&amp;quot; and &amp;quot;local&amp;quot; modules, in this case &amp;quot;local&amp;quot; will continue to be the current working directory where npm is run, and &amp;quot;global&amp;quot; will be in the &lt;code>lib&lt;/code> directory of the virtualenv you specified.&lt;/p>
&lt;p>Use the &lt;code>node&lt;/code> and &lt;code>npm&lt;/code> commands for manage Node in this environment further.&lt;/p>
&lt;h2>Ruby&lt;/h2>
&lt;p>Much like Node, some kind soul has provided a Python package for managing Ruby&amp;nbsp; versions. This may also be a good candidate for installing with the &lt;code>--user&lt;/code> flag if you&amp;#39;re gonig to use it often.&lt;/p>
&lt;pre>
&lt;code>pip install rubyenv&lt;/code>&lt;/pre>
&lt;p>Now you simply install the Ruby version of choice.&lt;/p>
&lt;pre>
&lt;code>rubyenv install 2.4.2&lt;/code>&lt;/pre>
&lt;p>Since it&amp;#39;s building from source you may need to install some system level packages for the build to succeed, but it should let you know.&lt;/p>
&lt;p>Now use the &lt;code>ruby&lt;/code> and &lt;code>gem&lt;/code> commands to setup and run whatever you need.&lt;/p>
&lt;h3>Go (Golang)&lt;/h3>
&lt;p>Go is a little more work to set up, but not much. There&amp;#39;s not a virtualenv aware tool, so you&amp;#39;ll need to download the &lt;a href="https://golang.org/dl/">Go Tools binaries&lt;/a>. Be sure to grab the appropriate tar.gz archive, not an installer, for your OS. Once you&amp;#39;ve got the archive, you just need to extract it into your virtualenv.&lt;/p>
&lt;pre>
tar -C $VIRTUAL_ENV -xvf download/go1.9.2.linux-amd64.tar.gz --strip 1&lt;/pre>
&lt;p>The &lt;code>-C $VIRTUAL_ENV&lt;/code> and &lt;code>--strip 1&lt;/code> are particularly important for the contents of the &lt;code>go&lt;/code> directory inside the archive to be extracted into you virtualenv correctly.&lt;/p>
&lt;p>Next, you need to tell Go where it&amp;#39;s based. The best way to do this is have virtualenv set the &lt;code>GOROOT&lt;/code> environment variable for you when you activate the virtualenv. If you only ever use Go inside a virtualenv, you can add the following to you user&amp;#39;s &lt;code>~/.virtualenv/postactivate&lt;/code>, then you only need to do the extraction step above when you create a new virtualenv. If you only want to use virtualenv Go for this one virtualenv, add the following to &lt;code>~/.virtualenv/&amp;lt;env name&amp;gt;/bin/postactivate&lt;/code>.&lt;/p>
&lt;pre>
GOROOT=&amp;quot;$VIRTUAL_ENV&amp;quot;
&lt;/pre>
&lt;p>Deactivate and reactivate your vritualenv and you&amp;#39;re all set. Manage everything using the &lt;code>go&lt;/code> command as usual.&lt;/p>
&lt;h3>Rust&lt;/h3>
&lt;p>Rust&amp;#39;s primary install tool, rustup, has the ability to install into custom paths. You just need to add a couple environment variables to tell it how, and we&amp;#39;ll let virtualenv activation handle that for us.&lt;/p>
&lt;p>Like Go, if you only use Rust within virtualenvs, and want to save a step in the future, you can add the following to &lt;code>~/.virtualenvs/postactivate&lt;/code>. If you just want to set up this one virtualenv, add it to &lt;code>~/.virtualenvs/&amp;lt;env name&amp;gt;/bin/postactivate&lt;/code>.&lt;/p>
&lt;pre>
&lt;code>CARGO_HOME=$VIRTUAL_ENV
RUSTUP_HOME=$VIRTUAL_ENV&lt;/code>
&lt;/pre>
&lt;p>Now&amp;#39;s a good time to deactivate and reactivate your virtualenv to load those variables. And we&amp;#39;re ready to run the rustup setup script.&lt;/p>
&lt;pre>
curl https://sh.rustup.rs -sSf | sh -s -- -y --no-modify-path&lt;/pre>
&lt;p>Done. Now &lt;code>rustc&lt;/code>, &lt;code>cargo&lt;/code> and all the other Rust commands should be good to go.&lt;/p>
&lt;h2>Conclusion&lt;/h2>
&lt;p>That&amp;#39;s it. Now you&amp;#39;ve got a pretty solid set of tools for developing different languages. They should all run side-by-side in any mixed and matched set you want to use and safely quarantined from the rest of your system.&lt;/p>
&lt;p>I&amp;#39;m sure there are several other languages that can be managed this way, and if I run across more I&amp;#39;ll try to update this post with details. Otherwise I hope this makes your development processes better.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Securing Your Website with Let&amp;#39;s Encrypt</title><link>https://jsatt.github.io/blog/securing-your-website-with-lets-encrypt/</link><pubDate>Sat, 23 Jul 2016 23:04:30 +0000</pubDate><guid>https://jsatt.github.io/blog/securing-your-website-with-lets-encrypt/</guid><description>
&lt;p>This past week I gave a short presentation at the monthly &lt;a href="http://tulsawebdevs.org/">TulsaWebDevs&lt;/a> meeting about setting up a secure website using &lt;a href="https://letsencrypt.org/">Let&amp;#39;s Encrypt&lt;/a>. I covered a brief (minimal) overview of how SSL/TLS works and a comparison in the processes of the traditional way to acquire an SSL certificate and acquiring a certificate via the ACME protocol. The slides are now up if you&amp;#39;d like to check them out over on my &lt;a href="../../../presentations/">Presentations page&lt;/a>. If you&amp;#39;d like to view my speakers notes simply press `s` while viewing the slideshow in Chrome.&lt;br />
&amp;nbsp;&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Securing Your Website With Let's Encrypt</title><link>https://jsatt.github.io/presentations/securing-your-website-with-lets-encrypt/</link><pubDate>Thu, 21 Jul 2016 05:32:32 +0000</pubDate><guid>https://jsatt.github.io/presentations/securing-your-website-with-lets-encrypt/</guid><description>
&lt;!doctype html>
&lt;html>
&lt;head>
&lt;meta charset="utf-8">
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
&lt;title>Securing Your Website With Let's Encrypt&lt;/title>
&lt;link rel="stylesheet" href="../../../media/presentations/letsencrypt/css/reveal.css">
&lt;link rel="stylesheet" href="../../../media/presentations/letsencrypt/css/theme/league.css">
&lt;!-- Theme used for syntax highlighting of code -->
&lt;link rel="stylesheet" href="../../../media/presentations/letsencrypt/lib/css/zenburn.css">
&lt;/head>
&lt;body>
&lt;div class="reveal">
&lt;div class="slides">
&lt;section>
&lt;h1>Securing Your Website&lt;/h1>
&lt;h2>With Let's Encrypt&lt;/h2>
&lt;h4>(In Under 5 Minutes)&lt;/h4>
&lt;/section>
&lt;section>
&lt;h2>Jeremy Satterfield&lt;/h2>
&lt;p>&lt;a href="http://jsatt.com">http://jsatt.com&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/jsatt">https://github.com/jsatt&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://twitter.com/jsatt">@jsatt&lt;/a>&lt;/p>
&lt;p>&lt;a mailto="jsatt@jsatt.com">jsatt@jsatt.com&lt;/a>&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Web developer for over 9 years.&lt;/li>
&lt;li>Worked for website measuring traffic in the millions/month&lt;/li>
&lt;li>currently working on internal software in oil industry&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;section>
&lt;h2>Primer on Protocols&lt;/h2>
&lt;h3 class="fragment">HTTP&lt;/h3>
&lt;h3 class="fragment">SSL/TLS&lt;/h3>
&lt;h3 class="fragment">HTTPS&lt;/h3>
&lt;h3 class="fragment">HTTP/2&lt;/h3>
&lt;aside class="notes">
&lt;ul>
&lt;li>HTTP - Hypertext Transfer Protocol, is the protocol of web&lt;/li>
&lt;li>SSL/TLS - Secure Sockets Layer and Transport Layer Security
&lt;ul>
&lt;li>TLS is evolution of SSL&lt;/li>
&lt;li>Both usually referred to as SSL&lt;/li>
&lt;li>cryptographic protocols that allow clients and servers to communicate securely&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HTTPS - HTTP over TLS, HTTP over SSL, HTTP Secure&lt;/li>
&lt;li>HTTP/2 - new version of http&lt;/li>
&lt;ul>
&lt;li>support by latest nginx and apache, slew of other web servers&lt;/li>
&lt;li>support by every modern browser, over TLS only&lt;/li>
&lt;li>other features out of scope&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;section>
&lt;h2>TLS Handshake&lt;/h2>
&lt;p>&lt;img title="http://rebecca.meritz.com/ggm15/#/19" src="../../../media/presentations/letsencrypt/img/tls_handshake.png" />&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Short: negotiate tls version, trade some random numbers, encrypt messages to verify, start talking application protocol&lt;/li>
&lt;li>Negotiation Phase
&lt;ul>
&lt;li>&lt;code>ClientHello&lt;/code> - client sends supported TLS Version, random number, supported ciphers and compression methods&lt;/li>
&lt;li>&lt;code>ServerHello&lt;/code> - server sends chosen TLS Version, random number, chosen ciphers and compression methods (based on client's)&lt;/li>
&lt;li>&lt;code>Certificate&lt;/code> - server send public key certificate&lt;/li>
&lt;li>&lt;code>ServerHelloDone&lt;/code> - server indicates handshake done&lt;/li>
&lt;li>&lt;code>ClientKeyExchange&lt;/code> - client sends &lt;code>PreMasterSecret&lt;/code> encrypted using server pub key&lt;/li>
&lt;li>Client and server use random numbers and &lt;code>PreMasterSecret&lt;/code> to calculate common "master secret"&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>ChangeCipherSpec&lt;/code>
&lt;ul>
&lt;li>client sends "I only talk encrpted now"&lt;/li>
&lt;li>client send encrypted &lt;code>Finished&lt;/code>&lt;/li>
&lt;li>server decrypts &lt;code>Finished&lt;/code>, if decrypt fails handshake fails and connection torn down&lt;/li>
&lt;li>server sencs "I only talk encrypted now"&lt;/li>
&lt;li>client decrypts &lt;code>Finished&lt;/code>, if decrypt fails handshake fails and connection torn down&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Application Phase - handshake done, application protocol (HTTP) enabled, application messages encrypted using same methods as &lt;code>Finished&lt;/code>&lt;/li>
&lt;li>Shutdown - after all messages sent, connection torndown&lt;/li>
&lt;/ul>
&lt;/section>
&lt;section>
&lt;section>
&lt;h2>Obtaining a Certificate&lt;/h2>
&lt;h3>The old way&lt;/h3>
&lt;ol>
&lt;li class="fragment">Prepare for Validation&lt;/li>
&lt;li class="fragment">Generate CSR&lt;/li>
&lt;li class="fragment">Order Certificate &lt;span class="fragment">$$$&lt;/span>&lt;/li>
&lt;li class="fragment">Have domain validated&lt;/li>
&lt;/ol>
&lt;aside class="notes">
&lt;ul>
&lt;li>Prepare
&lt;ul>
&lt;li>Expose WHOIS info for email confirmation&lt;/li>
&lt;li>Setting server for each domain for HTTP confirmation&lt;/li>
&lt;li>Adding CNAME for DNS confirmation&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Generate Certificate Signing Request
&lt;ul>
&lt;li>Encrypted details of certificate being requested&lt;/li>
&lt;li>Contains Common Name (domain), organization, location, contact email, public key used in certifcate&lt;/li>
&lt;li>Used to generate SSL certificate&lt;/li>
&lt;li>Also generates private key, hold on to it. Cert useless without it.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Order
&lt;ul>
&lt;li>register, fill out contact, submit CSR, Pay $$&lt;/li>
&lt;li>Single domain: $17-250/yr&lt;/li>
&lt;li>Multiple: $99-249/yr + per each additional&lt;/li>
&lt;li>Wildcard: $150-850/yr&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>wait for validate, usually "within minutes", reevaluate life&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;section data-background="http://i.giphy.com/l2R0corOGwFTlKZjO.gif">
&lt;/section>
&lt;section>
&lt;h2>Obtaining a Certificate&lt;/h2>
&lt;h3>The old way&lt;/h3>
&lt;ol>
&lt;li>Prepare for Validation&lt;/li>
&lt;li>Generate CSR&lt;/li>
&lt;li>Order Certificate $$$&lt;/li>
&lt;li>Have domain validated&lt;/li>
&lt;li>Receive and Install Certificate&lt;/li>
&lt;/ol>
&lt;aside class="notes">
&lt;ul>
&lt;li>Receive and install ticket&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;/section>
&lt;section>
&lt;h2>Let's Encrypt&lt;/h2>
&lt;section>
&lt;img src="../../../media/presentations/letsencrypt/img/letsencrypt.png" />
&lt;aside class="notes">
&lt;ul>
&lt;li>Non-profit ISRG Internet Secutiry Research Group&lt;/li>
&lt;li>Sponsors - EFF, Mozilla Foundation, Akamai, Cisco&lt;/li>
&lt;li>other partners - IdenTrust, Linux Foundation, Chrome, Stanford Law School, many others.&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;section>
&lt;h3>Pros&lt;/h3>
&lt;p class="fragment">Free&lt;/p>
&lt;p class="fragment">Automatic&lt;/p>
&lt;p class="fragment">Secure&lt;/p>
&lt;p class="fragment">Transparent&lt;/p>
&lt;p class="fragment">Open&lt;/p>
&lt;p class="fragment">Cooperative&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>free - to any domain owner&lt;/li>
&lt;li>Auto - uses software on server to verfiy, obtain, configure and renew certs&lt;/li>
&lt;li>Secure - promotes security best practices, help site ops to secure servers&lt;/li>
&lt;li>Transparent - all issued and revoked certs are public record&lt;/li>
&lt;li>Open - automatic issuance and renewal protocol is open standard, ACME&lt;/li>
&lt;li>Coop - joint effort from many orgs to benefit community, doesn't rely on one org&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;section>
&lt;h3>Cons&lt;/h3>
&lt;p class="fragment">Short Validity Time&lt;/p>
&lt;p class="fragment">No Wildcard Certificates&lt;/p>
&lt;p class="fragment">No Extended Validation or&lt;br> Organization Validation&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>short - expire in 90 days&lt;/li>
&lt;li>no wildcard - not big deal since certs are free, up to 100 domains on single cert via SAN&lt;/li>
&lt;li>no organization or extended validation - EV shows green "secure" bar&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;/section>
&lt;/section>
&lt;section>
&lt;h2>ACME&lt;/h2>
&lt;img src="../../../media/presentations/letsencrypt/img/acme.jpg" />
&lt;aside class="notes">
&lt;ul>
&lt;li>Automated Certificate Management Environment&lt;/li>
&lt;li>Protocol for CAs to interact with other CAs and users&lt;/li>
&lt;li>Allows Automated deployment&lt;/li>
&lt;li>JSON over HTTPS&lt;/li>
&lt;li>Standard currently in draft stage&lt;/li>
&lt;/ul>
&lt;/section>
&lt;section>
&lt;section>
&lt;h2>Obtaining a Certificate&lt;/h2>
&lt;h3>In the modern world&lt;/h3>
&lt;h4>Certbot&lt;/h4>
&lt;pre>&lt;code class="hlbash" data-trim contenteditable>
wget https://dl.eff.org/certbot-auto
chmod a+x certbot-auto
./certbot-auto certonly
&lt;/code>&lt;/pre>
&lt;aside class="notes">
&lt;ul>
&lt;li>stop nginx&lt;/li>
&lt;li>certonly for nginx&lt;/li>
&lt;li>all domains should be pointing to this IP&lt;/li>
&lt;li>start nginx&lt;/li>
&lt;/ul>
&lt;/section>
&lt;section data-background="http://i.giphy.com/90F8aUepslB84.gif">&lt;/section>
&lt;/section>
&lt;section>
&lt;h2>Installing Your Certificate&lt;/h2>
https://mozilla.github.io/server-side-tls/ssl-config-generator/
&lt;ul>
&lt;li>nginx&lt;/li>
&lt;li>Modern&lt;/li>
&lt;li>nignx 1.4.6&lt;/li>
&lt;li>openssl 1.0.1f&lt;/li>
&lt;/ul>
&lt;aside class="notes">
chained cert
&lt;/section>
&lt;section>
&lt;h2>More&lt;/h2>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">Wikipedia - Transport Layer Security&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://letsencrypt.org/">Let's Encrypt&lt;/a>&lt;/p>
&lt;p>
ACME -
&lt;a href="https://en.wikipedia.org/wiki/Automated_Certificate_Management_Environment">Wikipedia&lt;/a>
&lt;a href="https://github.com/ietf-wg-acme/acme">GitHub&lt;/a>
&lt;/p>
&lt;p>&lt;a href="http://rebecca.meritz.com/ggm15/">http://rebecca.meritz.com/ggm15/&lt;/a>&lt;/p>
&lt;/section>
&lt;/div>
&lt;/div>
&lt;script src="../../../media/presentations/letsencrypt/lib/js/head.min.js">&lt;/script>
&lt;script src="../../../media/presentations/letsencrypt/js/reveal.js">&lt;/script>
&lt;script>
// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
history: true,
// More info https://github.com/hakimel/reveal.js#dependencies
dependencies: [
{ src: '/media/presentations/letsencrypt/plugin/notes/notes.js', async: true },
{ src: '/media/presentations/letsencrypt/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
]
});
&lt;/script>
&lt;/body>
&lt;/html></description><dc:creator>Jeremy Satterfield</dc:creator></item><item><title>Unit Testing Recursion in Python</title><link>https://jsatt.github.io/blog/unit-testing-recursion-in-python/</link><pubDate>Thu, 11 Dec 2014 17:05:39 +0000</pubDate><guid>https://jsatt.github.io/blog/unit-testing-recursion-in-python/</guid><description>
&lt;p>Today I finally figured out the solution to a problem I&amp;#39;ve been trying to solve for a while. It&amp;#39;s kind of hacky and maybe a bad idea, but now I know it&amp;#39;s possible. The problem has always been that I&amp;#39;d like to test that a function recurses, but not needing it to actually have the recursion execute within to test. Just a unit test to assert that recursion is happening. After a little thought about how Python stores references I came up with this.&lt;/p>
&lt;pre class="language-python">
&lt;code>class ExampleTest(TestCase):
def setUp(self):
self.task = Task()
# ...
# other test stuff
# ...
def test_recursion(self):
original_func = self.task.run
self.mock.StubOutWithMock(Task, &amp;#39;run&amp;#39;)
Task.run(id=44)
self.mock.ReplayAll()
original_func(id=22)
self.mock.VerifyAll()
&lt;/code>&lt;/pre>
&lt;p>Because variables in Python are just references to objects in memory, if you create a new reference before stubbing out the primary reference, pointing it to a mock object, you can still access it via this new reference. You&amp;#39;re now free to check that recusion is happening, without the need for it to actually recurse in the test.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Keeping MRO In Mind When Mocking Inherited Methods</title><link>https://jsatt.github.io/blog/keeping-mro-in-mind-when-mocking-inherited-methods/</link><pubDate>Thu, 16 Oct 2014 19:33:11 +0000</pubDate><guid>https://jsatt.github.io/blog/keeping-mro-in-mind-when-mocking-inherited-methods/</guid><description>
&lt;p>So I just spent a couple of hours banging my head on a ridiculous PyMox mocking issue and though I&amp;#39;d share. Here is an example of the existing code.&lt;/p>
&lt;pre class="language-python">
&lt;code># views.py
class MyBaseView(BaseView):
def get_stuff(self):
# this is doing things
class SpecificView(MyBaseView):
example = True
def send_stuff(self):
stuff = self.get_stuff()
# do other things
# tests.py
class SpecificViewTest(TestCase):
def setUp(self):
self.view = SpecificView()
self.mox = mox.Mox()
def tearDown(self):
self.mox.UnsetStubs()
def test_send_stuff(self):
self.mox.StubOutWithMock(MyBaseView, &amp;#39;get_stuff&amp;#39;)
MyBaseView.get_stuff().AndReturn([&amp;#39;list&amp;#39;, &amp;#39;of&amp;#39;, &amp;#39;things&amp;#39;])
self.mox.ReplayAll()
self.view.send_stuff()
self.mox.VerifyAll()
# do assertions
&lt;/code>&lt;/pre>
&lt;p>This was all fine and working for months, until I added the following.&lt;/p>
&lt;pre>
&lt;code># views.py
class ExtraSpecificView(SpecificView):
def other_stuff(self):
self.get_stuff()
# do more things again
# test.py
class ExtraSpecificViewTest(TestCase):
def setUp(self):
self.view = ExtraSpecificView()
self.mox = mox.Mox()
def tearDown(self):
self.mox.UnsetStubs()
def test_other_stuff(self):
self.mox.StubOutWithMox(SpecifcView, &amp;#39;get_stuff&amp;#39;)
SpecificView.get_stuff().AndReturn([&amp;#39;more&amp;#39;, &amp;#39;things&amp;#39;])
self.mox.ReplayAll()
self.view.other_stuff()
self.mox.VerifyAll()
&lt;/code>&lt;/pre>
&lt;p>So what started happening at this point is &lt;code>SpecificViewTest.test_send_stuff&lt;/code> began failing with this.&lt;/p>
&lt;pre>
&lt;code>ExpectedMethodCallsError: Verify: Expected methods never called:
0. get_stuff.__call__() -&amp;gt; [&amp;#39;list&amp;#39;, &amp;#39;of&amp;#39;, &amp;#39;things&amp;#39;]&lt;/code>&lt;/pre>
&lt;p>It would only fail when &lt;code>ExtraSpecificViewTest&lt;/code> ran before &lt;code>SpecificViewTest&lt;/code> which, with nose, it does by default. So I knew it had to have something to do with the mocking in &lt;code>ExtraSpecificViewTest.test_other_stuff&lt;/code>. After more playing and banging my head I came up with this working theory which seems correct the more I think on it.&lt;/p>
&lt;p>When you mock out a method on a class, the mocking library stores the original method and replaces it with the mock function. When you tell the library to stop mocking the method, &lt;code>mox.UnsetStubs&lt;/code> in this case, it grabs that original method it stored before and assigns it back to the appropriate attribute on the class, restoring it&amp;#39;s original functionality.&lt;/p>
&lt;p>However, in my class I was mocking a method that &lt;code>SpecificView&lt;/code> inherited from &lt;code>MyBaseView&lt;/code>. When the library grabs the original method for storing Python sees that SpecificView doesn&amp;#39;t have a &lt;code>get_stuff&lt;/code> method of it&amp;#39;s own, so it moves up the &lt;abbr title="Method Resolution Order">MRO&lt;/abbr> and grabs &lt;code>MyBaseView.get_stuff&lt;/code> and the library stores that and assigns a mock function to the &lt;code>get_stuff&lt;/code> attribute on the &lt;strong>class&lt;/strong> of &lt;code>SpecificView&lt;/code>. Then, when the library goes to un-stub the method, it grabs the stored &lt;code>get_stuff&lt;/code> Python gave it from &lt;code>MyBaseView&lt;/code> and assigns it also to the &lt;code>get_stuff&lt;/code> attribute of the &lt;strong>class&lt;/strong> of &lt;code>SpecificView&lt;/code>. Finally, when &lt;code>SpecificViewTest.test_set_stuff&lt;/code> runs, &lt;code>send_stuff&lt;/code> calls &lt;code>SpecificView&lt;/code>&amp;#39;s &lt;code>get_stuff&lt;/code>, instead of moving up the MRO as to used to, Python now sees that the &lt;code>SpecificView&lt;/code> class has it&amp;#39;s own &lt;code>get_stuff&lt;/code> method (which doesn&amp;#39;t have a super call), so it doesn&amp;#39;t move up the MRO, therefore &lt;code>MyBaseView&lt;/code> is never called as expected and raising a failure when verified.&lt;/p>
&lt;p>So this became the fix.&lt;/p>
&lt;pre>
&lt;code>#tests.py
class ExtraSpecificViewTest(TestCase):
...
def test_other_stuff(self):
self.mox.StubOutWithMox(MyBaseView, &amp;#39;get_stuff&amp;#39;)
MyBaseView.get_stuff().AndReturn([&amp;#39;more&amp;#39;, &amp;#39;things&amp;#39;])
&lt;/code>&lt;/pre>
&lt;p>It&amp;#39;s a convoluted bug that may or may not be specific to PyMox or other mocking libraries, but I&amp;#39;m not sure how they should be properly handling something like this. So I guess the moral of the story is to mock methods from the class which the were originally defined, not just the class you inherited from.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Mocking a property in Python</title><link>https://jsatt.github.io/blog/mocking-a-property-in-python/</link><pubDate>Fri, 08 Aug 2014 21:10:45 +0000</pubDate><guid>https://jsatt.github.io/blog/mocking-a-property-in-python/</guid><description>
&lt;p>Anytime I see someone turning an instance method into a property on a Python object, I always have to step back and rethink whether it&amp;#39;s really the right thing to do. While properties certain have valid use cases, I often see them overused and misused. This can result in code that is harder to refactor should you decide you actually do want to accept arguments as well as less straight forward to separate in unit tests.&lt;/p>
&lt;p>The way I found to deal with the latter case is pretty simple but unituitive at first. Simply stub out the property on the class, then assign the property value on the instance.&lt;/p>
&lt;pre class="language-python">
&lt;code># Use __class__ if you don&amp;#39;t have the class imported
self.mox.StubOutWithMock(self.book.__class__, &amp;#39;reviews&amp;#39;)
self.book.editors = [&amp;#39;editor 1&amp;#39;, &amp;#39;editor 2&amp;#39;]
# replace with a MockAnything to go deeper
self.mock.StubOutWithMock(Company, &amp;#39;employees&amp;#39;)
Company.employees = self.mock.CreateMockAnything()
company.employees.are_executives().AndReturn([&amp;#39;CEO&amp;#39;, &amp;#39;COO&amp;#39;, &amp;#39;CIO&amp;#39;])
campaign3.reviews.are_developers().AndReturn([&amp;#39;Jim&amp;#39;, &amp;#39;John&amp;#39;, &amp;#39;Joey&amp;#39;])&lt;/code>&lt;/pre></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Class-based Celery Tasks</title><link>https://jsatt.github.io/blog/class-based-celery-tasks/</link><pubDate>Thu, 05 Jun 2014 23:25:06 +0000</pubDate><guid>https://jsatt.github.io/blog/class-based-celery-tasks/</guid><description>
&lt;p>Update 2017-11-02: Celery 4 now &lt;a href="http://docs.celeryproject.org/en/latest/whatsnew-4.0.html#the-task-base-class-no-longer-automatically-register-tasks">adivises against inheriting from Task&lt;/a> unless you are extending common functionality. However you can still get similar functionality by creating a new class and calling is from inside a decorated function task.&lt;/p>
&lt;pre>
&lt;code>class MyTask(object):
def run(self, source):
do_stuff()
...
@app.task
def my_task(source):
MyTask().run(source)
&lt;/code>&lt;/pre>
&lt;p>Original Post:&lt;/p>
&lt;p>Recently, I&amp;#39;ve had to write several complicated Celery tasks. Unfortunately, when doing a complicated process standard task functions can become unwieldy to write, read and unit test. After looking into how Celery tasks actually work, I was able to find a more manageable way of writing these complex tasks.&lt;/p>
&lt;p>It turns out the &lt;code>task&lt;/code> decorator that you&amp;#39;re used to using is just an object factory for &lt;code>Task&lt;/code> objects that turns the decorated function into the &lt;code>run&lt;/code> method of on the &lt;code>Task&lt;/code> instance it creates. Skipping the decorator and extending the &lt;code>Task&lt;/code> class directly makes things a little more flexible.&lt;/p>
&lt;pre>
&lt;code class="language-python">from celery import Task
class MyTask(Task):
ignore_result = True
def run(self, source, *args, **kwargs):
self.source = source
data = self.collect_data()
self.generate_file(data)
def generate_file(self, data):
# do your file generation here
...
def collect_data(self):
# do your data collection here
...
return data&lt;/code>
&lt;/pre>
&lt;p>In this case &lt;code>run&lt;/code> is the equivalent of the function task you&amp;#39;re used to, but thanks to OOP you&amp;#39;re free to break some of the more complex code into logic blocks, &lt;code>collect_data&lt;/code> and &lt;code>generate_file&lt;/code>, and access to instance attribute, &lt;code>source&lt;/code>.&lt;/p>
&lt;p>To call the task your just need to instantiate the it and call the desired method to trigger it.&lt;/p>
&lt;pre>
&lt;code class="language-python">task = MyTask()
# Run on Celery worker now
task.delay(123)
# Run on Celery worker at a sepcfic time
task.apply_async(args=[123], eta=datetime(2015, 6, 5, 12, 30, 22))
# Run task directly, No celery worker
task.run(123)
task(123)
&lt;/code>&lt;/pre>
&lt;p>Testing also now becomes easier as well since you can test each unit on it&amp;#39;s own.&lt;/p>
&lt;p>For most cases, your standard function-based classes are probably going to do the job. But for those extra complex cases, class-based might make things easier work with.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Mocking Python&amp;#39;s built-in open function</title><link>https://jsatt.github.io/blog/mocking-pythons-built-in-open-function/</link><pubDate>Fri, 11 Apr 2014 17:25:38 +0000</pubDate><guid>https://jsatt.github.io/blog/mocking-pythons-built-in-open-function/</guid><description>
&lt;p>A while back I was working on my &lt;a href="https://github.com/jsatt/django-podcast-client">podcast client&lt;/a> and ran into an issue downloading large files. The root problem was that all of Django&amp;#39;s &lt;code>FileField&lt;/code> backends store the file (or file-like object) to memory then saves it to disk, and the cubieboard system I was using had limitied memry resources, resulting in &amp;quot;out of memory&amp;quot; errors. After much searching and hacking I finally settled on just storing the file to disk myself using &lt;a href="http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow">requests streaming argument&lt;/a>. This allowed me to download the file in chunks and save directly to disk and then tell the Django field where I placed it, as you can see &lt;a href="https://github.com/jsatt/django-podcast-client/blob/master/podcast_client/models.py#L131-L148">here&lt;/a>.&lt;/p>
&lt;p>Then I went to update tests. This is when a new problem presented itself. I wanted to test that proper calls were being made to store the file to disk, but I didn&amp;#39;t want to store a file to disk during tests. Mocking never worked the way you&amp;#39;d expect for the built-in open. I placed it on the back burner for a while until I ran into a similar issue an another project. When I finally found the solution, it was incredibly simple and incredibly nonobvious.&lt;/p>
&lt;p>Pythons has a bunch of built-in functions that are just preloaded, such as &lt;code>open&lt;/code> for opening files, &lt;code>print&lt;/code> for outputting to the console, &lt;code>str&lt;/code>, &lt;code>bool&lt;/code>, &lt;code>dict&lt;/code>, &lt;code>list&lt;/code>, etc. for casting or creating an object, among others. Most of these just do what they do, with no affect on the overall environment; therefore, when testing I just let them do what they do and mocking them is probably a bad idea anyway. Except for &lt;code>open&lt;/code>, it has real world affects on the environment in that it accesses, and allows writing of, files on the disk.&lt;/p>
&lt;p>Finally, I stumbled across the &lt;code>__builtin__&lt;/code> module. This is the magic module where all those functions actually reside, and if you can access where a function resides you can mock it out. The following should work on most Python mocking frameworks, but this is how to use PyMox to do it.&lt;/p>
&lt;pre>
&lt;code class="language-python">class Client(APIClient):
...
def open_file(self):
file = open(self.path, &amp;#39;r&amp;#39;)
contents = file.read()
file.close()
return self.prepare_file(contents)
...
# test.py
import __builtin__
from StringIO import StringIO
class ClientTest(TestCase):
...
def test_open_file(self):
self.mock.StubOutWithMock(__builtin__, &amp;#39;open&amp;#39;)
mock_content = StringIO(&amp;#39;test&amp;#39;)
self.mock.StubOutWithMock(mock_content, &amp;#39;close&amp;#39;)
open(&amp;#39;/tmp/mypath.txt&amp;#39;, &amp;#39;r&amp;#39;).AndReturn(mock_content)
mock_content.close()
self.mock.ReplayAll()
file = self.client.open_file()
self.mock.VerifyAll()
...&lt;/code>
&lt;/pre>
&lt;p>In this case, returning an instance of StringIO results in a file-like object allowing me to set the contents of the &amp;quot;file&amp;quot; being accessed directly in the test. No fixtures or creating files on the disk. This pattern works for both reads and writes.&lt;/p>
&lt;p>But the code I&amp;#39;m testing here doesn&amp;#39;t do all the error handling to make sure the file is closed even if there&amp;#39;s an error during the read. Frankly, they made it possible to use &lt;code>open&lt;/code> as a context manager so that you don&amp;#39;t have to worry about it. So combining this new knowledge with my previous post about &lt;a href="../../../blog/mocking-context-managers-in-python/">mocking context managers&lt;/a>, we can easily write cleaner code that&amp;#39;s still testable.&lt;/p>
&lt;pre>
&lt;code class="language-python">class Client(APIClient):
...
def write_file(self):
with open(self.path, &amp;#39;wb&amp;#39;) as f:
f.write(self.content)
...
# test.py
class ClientTest(TestCase):
...
def test_write_file(self):
self.mock.StubOutWithMock(__builtin__, &amp;#39;open&amp;#39;)
mock_file1 = self.mock.CreateMockAnything()
open(&amp;#39;/tmp1/mypath.txt&amp;#39;, &amp;#39;wb&amp;#39;).AndReturn(mock_file1)
mock_file1.__enter__().AndReturn(mock_file1)
mock_file1.write(&amp;#39;this is my content&amp;#39;)
mock_file1.__exit__(None, None, None)
self.mock.ReplayAll()
self.client.write_file()
self.mock.VerifyAll()
...&lt;/code>
&lt;/pre>
&lt;p>Since StringIO can&amp;#39;t be used as a context manager, this pattern doesn&amp;#39;t allow for the use of StringIO. This means you have to mock out the reads, writes and any other methods you would call on a file-like object, but I think that&amp;#39;s a small price for being able to write cleaner, testable code.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Abusing Django Rest Framework Part 4: Object-level field exclusion</title><link>https://jsatt.github.io/blog/abusing-django-rest-framework-part-4-object-level-field-exclusion/</link><pubDate>Thu, 27 Mar 2014 16:55:11 +0000</pubDate><guid>https://jsatt.github.io/blog/abusing-django-rest-framework-part-4-object-level-field-exclusion/</guid><description>
&lt;p>Similar to the object-level readonly field from my &lt;a href="../../../blog/abusing-django-rest-framework-part-3-object-level-read-only-fields/">previous post&lt;/a>, there are some cases where we want to exclude certain fields based on what object the user is trying to access. You could overwrite the views &lt;code>get_serializer&lt;/code> method to use a different serializer based on their access, but if nesting serializers is a possiblility this get messy, somewhere in the neighborhood of &lt;a href="http://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/" target="_blank">O(n&lt;sup>2&lt;/sup>)&lt;/a>. Another option is to modify a serializers &lt;code>to_native&lt;/code> method.&lt;/p>
&lt;pre>
&lt;code class="language-python">class SampleSerializer(serializers.ModelSerializer):
...
def to_native(self, obj):
restricted = not obj.review_set.filter(
client__plan__account_details=True).exists()
if restricted:
self.fields.pop(&amp;#39;billing_address&amp;#39;, None)
return super(SampleSerializer, self).to_native(obj)&lt;/code>&lt;/pre>
&lt;p>The &lt;code>to_native&lt;/code> method is responsible for turning the Python object into a simple, serializable value. This means it has to be called for every object that it being retrieved, so it will always be able to affect your output, even when nested.&lt;/p>
&lt;p>One drawback is that it only works in conjunction with retrieve views. If the field is defined in the serializer at any point as writeable, the users can still update that field, even if they can&amp;#39;t see it. This can be a pretty big security risk when used with update views.&lt;/p>
&lt;h2>Another Approach&lt;/h2>
&lt;p>Having implemented this method several months ago, based on a spec that was several months old at the time, I would likely approach the root issue from a different angle if it came up again. Maybe creating updating the spec to have a consistant api response and return blank values for the unauthorized fields would be a better approach. &lt;code>SerializerMthodField&lt;/code> has the potential to make this approach much cleaner.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Abusing Django Rest Framework Part 3: Object-level read-only fields</title><link>https://jsatt.github.io/blog/abusing-django-rest-framework-part-3-object-level-read-only-fields/</link><pubDate>Wed, 26 Mar 2014 16:25:04 +0000</pubDate><guid>https://jsatt.github.io/blog/abusing-django-rest-framework-part-3-object-level-read-only-fields/</guid><description>
&lt;p>DRF has tools to control access in a few ways. Serializers make it easy to select what fields can be accessed and whether or not they are read-only. Permissions are great for restricting access to objects at all or even making certain objects read-only. But there are also cases where you might only want to allow access to a field on a specific object but leave that field restricted on other objects, or vice-versa.&lt;/p>
&lt;p>In our case, we had an endpoint for modifying users, which has a field to dictate whether the user is an account admin. Our view already uses permissions to verify that the request user is allow to modify these items at all, but we wanted to make sure the user couldn&amp;#39;t accidentally remove their own admin access but still be able to change other fields on this endpoint.&lt;/p>
&lt;p>Whenever a &lt;code>Serializer&lt;/code> is instantiated, the &lt;code>get_fields&lt;/code> method is called to look at the attributes and decide what fields to include and instantiate them. As part of the serialzer instantiation the view also passes the request, view and &lt;code>format_kwarg&lt;/code> as context to the serializer which is attached to the instance. This means we can use the request or view, or objects attached to them in to change attributes of the fields, in our case making one of them readonly.&lt;/p>
&lt;pre>&lt;code class="language-python">class SampleSerializer(serializers.ModelSerializer):
...
def get_fields(self, *args, **kwargs):
fields = super(SampleSerializer, self).get_fields(*args, **kwargs)
request = self.context.get(&amp;#39;request&amp;#39;, None)
view = self.context.get(&amp;#39;view&amp;#39;, None)
if (request and view and getattr(view, &amp;#39;object&amp;#39;, None) and
request.user == view.object.user):
fields[&amp;#39;is_admin&amp;#39;].read_only = True
return fields&lt;/code>&lt;/pre>
&lt;p>The one issue with this method is that you can not nest this serializer and have this change work. Whenever a Serializer is nested on to another, the parent gets one instance of this child at the time that the server process starts, which gets reused for all future instances of said parent. Due to lots of complicated handling on DRF&amp;#39;s part, that I&amp;#39;ll leave to you to read up on, everything just works the way you expect most of the time. However, we are trying to access the request and view objects that clearly don&amp;#39;t exist at the time the server process starts.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Using Tox with Travis CI to Test Django Apps</title><link>https://jsatt.github.io/blog/using-tox-with-travis-ci-to-test-django-apps/</link><pubDate>Sun, 09 Mar 2014 01:18:20 +0000</pubDate><guid>https://jsatt.github.io/blog/using-tox-with-travis-ci-to-test-django-apps/</guid><description>
&lt;p>Being a fan of good testing, I&amp;#39;m always trying to find ways to improve testing on various projects. &lt;a href="https://travis-ci.org/">Travis CI&lt;/a> and &lt;a href="https://coveralls.io/">Coveralls&lt;/a> are really nice ways to set up continuous integration for your open-source projects. A couple months ago I finally started hearing grumblings about &lt;a href="http://testrun.org/tox/latest/index.html">tox&lt;/a> and how everyone was it using for their Python test automation. Every time I&amp;#39;d try to wrap my head around it, something always eluded me, so this week I finally decided to dive in head first and see if I could get to the bottom of it and how it could improve my current integration setup.&lt;/p>
&lt;h2>Headless Django Environments&lt;/h2>
&lt;p>Most of my open-source Python projects have been developed from the beginning to be installable modules for use with tools like pip. With Django apps, this tends to be a little more difficult as Django development environments generally assumes you have a project that contains a lot boilerplate (manage.py, project/settings.py, project/urls.py), but some of this stuff is just clutter in an installable package. After trying several patterns, I found the one used by &lt;a href="http://jamessocol.com/">James Socol&lt;/a> on several projects, using &lt;a href="http://docs.fabfile.org/en/1.8/">fabric&lt;/a> to wrap my development enviroment and easily provide some of the common commands we use manage.py for, but without all the clutter of a full Django project. I&amp;#39;ve also created a repository that&amp;#39;s intended to be used as a Django &lt;a href="https://docs.djangoproject.com/en/dev/ref/django-admin/#startproject-projectname-destination">project template&lt;/a>, to help with setting out a project like this. It even includes some of the setup I&amp;#39;m about to discuss. You can get that template &lt;a href="https://github.com/jsatt/django-installable-plugin-template">over at Github&lt;/a>.&lt;/p>
&lt;h2>Preparing tox&lt;/h2>
&lt;p>Start by installing tox with a simple &lt;code>pip install tox&lt;/code>. Then create the tox.ini file in the base of your project. The first section you care about is configuring tox itself.&lt;/p>
&lt;pre>&lt;code class="language-none">
[tox]
envlist = django15, django16&lt;/code>&lt;/pre>
&lt;p>The &lt;code>envlist&lt;/code> directive tells tox which environments to run by default when calling it from the CLI. Tox comes with a few built-in environments (py26, py27, py31, py33, etc.) for testing different Python versions, but in my case I care about testing different Django versions, so I&amp;#39;ll be building my own environments.&lt;/p>
&lt;pre>&lt;code class="language-none">[testenv]
commands = django-admin.py test
setenv =
DJANGO_SETTINGS_MODULE=test_app.settings
PYTHONPATH={toxinidir}&lt;/code>&lt;/pre>
&lt;p>&lt;code>testenv&lt;/code> is the set of default environment settings that all named environments will inherit from. Here I&amp;#39;m telling it that the default command I&amp;#39;m going to use is &lt;code>django-admin.py test&lt;/code> to run tests. And setup environment variables that are usually handled by &lt;code>manage.py&lt;/code>. Tox also allows for substituting variables, in this case &lt;code>{toxinidir}&lt;/code> is simply the path of the directory which contains the &lt;code>tox.ini&lt;/code> file. It also allows for doing substitutions based on lookups within the ini file. By defining a custom section, we can define settings that can be reused and extended within our environments.&lt;/p>
&lt;pre>&lt;code class="language-none">[base]
deps =
mox
nose
django-nose&lt;/code>&lt;/pre>
&lt;p>Here I&amp;#39;m defining some dependencies that I&amp;#39;m going to add to in each environment. In my case, I prefer Nose and Mox to their standard library counterparts.&lt;/p>
&lt;pre>&lt;code class="language-none">[testenv:django15]
deps =
django&amp;gt;=1.5, &amp;lt;1.6
{[base]deps}
[testenv:django16]
deps =
django&amp;gt;=1.6, &amp;lt;1.7
{[base]deps}&lt;/code>&lt;/pre>
&lt;p>This creates two new test environments, named django15 and django16, and adds the dependency to install the proper version of Django, 1.5 and 1.6 respectively. It also uses the section lookup to append the deps from the base section we defined above. Now, when you return to the CLI and run tox, you should see it setup virtualenvs for each environment and run your tests under them. That&amp;#39;s all it take to get it running. Anyone can now clone your repository and simply run tox to confirm that tests are passing. This is particularly useful if they intend to contribute back to the project.&lt;/p>
&lt;h3>Adding Coverage&lt;/h3>
&lt;p>Now, as I mentioned before, I want to also use Coveralls to show coverage stats as well. To do that I just need to define another environment.&lt;/p>
&lt;pre>&lt;code class="language-none">[testenv:coverage]
commands =
coverage run --branch --omit={envdir}/*,test_app/*.py,*/migrations/*.py {envbindir}/django-admin.py test
coveralls
deps =
coverage
coveralls
{[testenv:django16]deps}&lt;/code>&lt;/pre>
&lt;p>This time I&amp;#39;m running &lt;code>coverage run&lt;/code> with branch coverage, ignoring migrations and my test app and running the &lt;code>django-admin.py&lt;/code> in the enviroments bin directory, followed by the coveralls reporting script. We also have a couple of additional dependencies to add to the django16 deps.&lt;/p>
&lt;h2>Running Tox on Travis&lt;/h2>
&lt;p>With tox configured to handle the details, Travis just needs to know about the different environments. Here&amp;#39;s what the &lt;code>.travis.yaml&lt;/code> looks like.&lt;/p>
&lt;pre>&lt;code class="language-yaml">language: python
install:
- pip install tox
script:
- tox
env:
- TOXENV=django15
- TOXENV=django16
- TOXENV=coverage&lt;/code>&lt;/pre>
&lt;p>This tells Travis to install tox and to run the &lt;code>tox&lt;/code> command. Adding the &lt;code>TOXENV&lt;/code> environment variable makes it easy to specify which testenv to run.Travis runs it&amp;#39;s tasks based on the build matrix of environment variables and language versions, allowing us to run each testenv asynchronously and reporting the results for each version separately.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Mocking Context Managers in Python</title><link>https://jsatt.github.io/blog/mocking-context-managers-in-python/</link><pubDate>Fri, 28 Feb 2014 22:33:13 +0000</pubDate><guid>https://jsatt.github.io/blog/mocking-context-managers-in-python/</guid><description>
&lt;p>I&amp;#39;ve often found Python&amp;#39;s context managers to be pretty useful. They make a nice interface that can handle starting and ending of temporary things for you, like opening and closing a file.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>
&lt;code class="language-python">f = open(&amp;#39;myfile.txt&amp;#39;, &amp;#39;w&amp;#39;)
try:
for row in records:
f.write(row)
finally:
f.close()&lt;/code>&lt;/pre>
&lt;p>can be replaced with&lt;/p>
&lt;pre>
&lt;code class="language-python">with open(&amp;#39;myfile.txt&amp;#39;, &amp;#39;w&amp;#39;) as f:
for row in records:
f.write(row)&lt;/code>&lt;/pre>
&lt;p>This last week I was working with the ZipFile module and wanted to use it&amp;#39;s context manger interface, but I ran into a little confusion when it came to unit testing. After a little better understanding of how context managers work, I figured out that the &lt;code>__enter__&lt;/code> and &lt;code>__exit__&lt;/code> methods are what really makes a context handler. As &lt;a href="http://pymotw.com/2/contextlib/">explained at PyMOTW&lt;/a>, when you invoke &lt;code>with&lt;/code> on a class, __enter__ is called and should return an object to be used in the context (&lt;code>f&lt;/code> in the above example), the code within the block is executed, and &lt;code>__exit__&lt;/code> is called no matter the outcome of the block. So both of these would be roughly equivalent, assuming &lt;code>do_stuff&lt;/code> doesn&amp;#39;t raise an exception.&lt;/p>
&lt;pre>
&lt;code class="language-python">with Context(foo) as bar:
do_stuff(bar)
c = Context(foo)
bar = c.__enter__()
try:
do_stuff(bar)
finally:
c.__exit__(None, None, None)&lt;/code>&lt;/pre>
&lt;p>With this understanding, here is the solution to my mocking problem using PyMox.&lt;/p>
&lt;pre>
&lt;code class="language-python">#module/tasks.py
def zip_it_up(filename):
with ZipFile(filename, &amp;#39;w&amp;#39;) as f:
for file in FILES:
f.write(file.path, file.name)
# tests.py
... # setup and stuff is up here somewhere
def test_building_zipfile(self):
self.mock.StubOutWithMock(module.tasks, &amp;#39;ZipFile&amp;#39;)
&amp;nbsp;&amp;nbsp;&amp;nbsp; mock_zip = self.mock.CreateMockAnything()
&amp;nbsp;&amp;nbsp;&amp;nbsp; module.tasks.ZipFile(&amp;#39;/tmp/export.zip&amp;#39;, &amp;#39;w&amp;#39;).AndReturn(mock_zip)
&amp;nbsp;&amp;nbsp;&amp;nbsp; mock_zip.__enter__().AndReturn(mock_zip)
&amp;nbsp;&amp;nbsp;&amp;nbsp; mock_zip.write(&amp;#39;/tmp/export-1.xml&amp;#39;, &amp;#39;export-1.xml&amp;#39;)
&amp;nbsp;&amp;nbsp;&amp;nbsp; mock_zip.write(&amp;#39;/tmp/export-2.xml&amp;#39;, &amp;#39;export-2.xml&amp;#39;)
&amp;nbsp;&amp;nbsp;&amp;nbsp; mock_zip.__exit__(None, None, None)
self.mock.ReplayAll()
zip_it_up()
self.mock.VerifyAll()&lt;/code>&lt;/pre>
&lt;p>Mocking out ZipFile allows us to return a mock object from it&amp;#39;s instantiation. We can then set the expectation that __enter__ will be called on the instance, returning the instance itself, expecting &lt;code>write&lt;/code> to be called twice on the instance and finally __exit__ to be called. The three arguments of &lt;code>None&lt;/code> here are to indicate that an exception isn&amp;#39;t expected. If the code inside the context block were to raise an exception, these arguments would be the &lt;code>type&lt;/code>, &lt;code>value&lt;/code> and &lt;code>traceback&lt;/code> as returned by &lt;code>raise&lt;/code>. In the event you are testing for an exception, these arguments should be set accordingly when setting expectations.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Abusing Django Rest Framework Part 2: Non-rate-based Throttling</title><link>https://jsatt.github.io/blog/abusing-django-rest-framework-part-2-non-rate-based-throttling/</link><pubDate>Wed, 19 Feb 2014 16:43:17 +0000</pubDate><guid>https://jsatt.github.io/blog/abusing-django-rest-framework-part-2-non-rate-based-throttling/</guid><description>
&lt;p>Anyone running an API that can be reached by the outside world should most definitely be concerned that someone might pummel their server by making a massive amount of requests to that one endpoint that requires a bunch of on-the-fly calculations. Enter Django Rest Framework&amp;#39;s throttling. It allows you to easily configure the framework to stop allowing requests from a user once they&amp;#39;ve made so many requests in a period of time. Whether you&amp;#39;re concerned about requests over a sustained period of time or in short bursts, rate limiting with throttles will handle it.&lt;/p>
&lt;p>However, there are times you might need to limit based on something less consistent than number of calls per minute or day. One example we we ran into was clients submitting requests for a process that requires a lot of human work to complete, and the possibility that a large number of these requests might mean that the client doesn&amp;#39;t actually understand the real purpose of the feature, or an indicator of a bigger problem for that client. If you can limit these clients to a certain number of outstanding requests at any given time, this allows for the time for our client contacts to process the appropriate requests or contact them if the requests are inappropriate.&lt;/p>
&lt;p>While the throttles provided by Rest Framework are all about rate limiting, writing a custom throttle to do this is pretty simple.&lt;/p>
&lt;pre>&lt;code class="language-python">### throttles.py
from django.db.models import Count
from rest_framework.throttling import BaseThrottle
from rest_framework.exceptions import Throttled
from .models import Inspection
class InspectionThrottle(BaseThrottle):
def allow_request(self, request, view):
inspections = Inspection.object.filter(client=view.client)
if inspections &amp;lt; 15:
return True
raise Throttled(detail=(
&amp;quot;You have reached the limit of 15 open requests. &amp;quot;
&amp;quot;Please wait until your existing requests have been &amp;quot;
&amp;quot;evaluated before submitting additional disputes. &amp;quot;))
###views.py
class InspectionCreateView(CreateAPIView):
model = Inspection
throttle_classes = InspectionThrottle&lt;/code>&lt;/pre>
&lt;p>By simply defining an &lt;code>allow_request&lt;/code> method you are free to use whatever logic you want to throttle requests, so long as you return &lt;code>True&lt;/code> to allow the request and raise the &lt;code>Throttled&lt;/code> exception to deny the request. &lt;code>Throttled&lt;/code> also accepts a kwarg of wait which will set the &lt;code>X-Throttle-Wait-Second&lt;/code> header for notifying the client how long to wait before trying again. This is mostly just useful for rate limiting, but maybe you&amp;#39;ll be clever and have a reason to use it.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>About</title><link>https://jsatt.github.io/about/</link><pubDate>Mon, 10 Feb 2014 22:36:53 +0000</pubDate><guid>https://jsatt.github.io/about/</guid><description>
&lt;div class="row">
&lt;div class="col-md-4 pull-right">&lt;img src="../../../media/me-again.jpg" style="float:right; height:364px; width:300px" title="Jeremy Satterfield" />&lt;/div>
&lt;div class="col-md-8">
&lt;p>I&amp;#39;m a web developer and homebrewer from Tulsa, OK with an affinity toward open-source. I work primarily in Python, Django, Javascript and and related languages such as CoffeeScript, CSS and Stylus. As a member of &lt;a href="http://tulsawebdevs.org" target="_blank">TulsaWebDevs&lt;/a>, &lt;a href="http://codefortulsa.org" target="_blank">CodeforTulsa&lt;/a>, and &lt;a href="http://civicninjas.org">Civic Ninjas&lt;/a>, I also work on many civic-minded projects.&lt;/p>
&lt;/div>
&lt;/div></description><dc:creator>Jeremy Satterfield</dc:creator></item><item><title>Abusing Django Rest Framework Part 1: Non-Model Endpoints</title><link>https://jsatt.github.io/blog/abusing-django-rest-framework-part-1-non-model-endpoints/</link><pubDate>Wed, 29 Jan 2014 18:32:43 +0000</pubDate><guid>https://jsatt.github.io/blog/abusing-django-rest-framework-part-1-non-model-endpoints/</guid><description>
&lt;p>When working with &lt;a href="http://www.django-rest-framework.org/" target="_blank">Django Rest Framwork&lt;/a> a few months back, there were a few road blocks that we ran into. Rest Framwork is awesome with most models for providing a simple CRUD API, in any (or multiple) serializations, with authentication and permissions. Sometimes, however, things aren&amp;#39;t so simple. Things get ugly. Framworks get abused.&lt;/p>
&lt;p>This is the first of a few posts looking at some of the ways Rest Framwork can be beaten into submission. Some of them may be the way the framework intended them to be implement, but non-obvious, while others were never intended, are probably bad ideas, and should be avoided at all costs (except when the needs of the business require it, of course).&lt;/p>
&lt;h2>Creating Endpoints to Trigger Non-model Processes&lt;/h2>
&lt;p>Let&amp;#39;s start out easy. As I mentioned Rest Framework is great at working with models, but it can also be easily used for things other than retrieving and saving models. Say for instance you have an email form on your site for sharing a post. Using AJAX posting of those fields makes for a better experience for your user and keeps them browsing the page longer.&lt;/p>
&lt;p>While it&amp;#39;s not exactly clear, the &lt;code>APIView&lt;/code> allows for this. Most of the documentation for &lt;code>APIView&lt;/code> is spent doing model retrievals that are made easier when you start using &lt;code>GenericAPIView&lt;/code> and those that inherit from it. However, using the dispatch methods (&lt;code>post&lt;/code>, &lt;code>get&lt;/code>, &lt;code>delete&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>put&lt;/code>, etc.), just like a Django class-based view, our example above becomes pretty easy.&lt;/p>
&lt;pre>
&lt;code class="language-python">from rest_framework import views
from rest_framework.response import Response
class ShareView(views.APIView):
permission_classes = []
def post(self, request, *args, **kwargs):
email = request.DATA.get(&amp;#39;email&amp;#39;, None)
url = request.DATA.get(&amp;#39;url&amp;#39;, None)
if email and url:
share_url(email, url)
return Response({&amp;quot;success&amp;quot;: True})
else:
return Response({&amp;quot;success&amp;quot;: False})&lt;/code>&lt;/pre>
&lt;p>In these dispatch methods you can easily access the deserialized posted data (via &lt;code>request.DATA&lt;/code>), run your custom code (&lt;code>share_url()&lt;/code> in this case) and return a serialized response (passing your data to &lt;code>Response&lt;/code>). If you&amp;#39;re arbitrary code execution applies to a model object, maybe a method call, but doesn&amp;#39;t including retrieving or updating that model, simply inherit from the &lt;code>GenericAPIView&lt;/code> instead the &lt;code>APIView&lt;/code> and use the &lt;code>get_object&lt;/code> method of the view.&lt;/p>
&lt;p>The one caveat when using &lt;code>APIView&lt;/code> is to watch what permissions are applied to the view. Since it is not associated with a model, any permission that expects a model object to be available, like the &lt;code>DjangoModelPermission&lt;/code> that is applied by default, will fail, so you should be explicit about permissions.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Finding Un-mocked HTTP requests in Python tests with Nose</title><link>https://jsatt.github.io/blog/finding-un-mocked-http-requests-in-python-tests-with-nose/</link><pubDate>Thu, 16 Jan 2014 18:40:58 +0000</pubDate><guid>https://jsatt.github.io/blog/finding-un-mocked-http-requests-in-python-tests-with-nose/</guid><description>
&lt;p>I&amp;#39;m a big fan of proper unit testing with mocking. So I&amp;#39;m pretty disappointed when we run across a unit that is not only not mocked properly, but results in real-world consequences outside the testing environment. One case we&amp;#39;ve run into couple of times now is tests that are making actual outbound HTTP requests to remote servers. Since clients don&amp;#39;t necessarily like getting fake information posted to their servers every time you run tests, I figured this was a good time to get to the bottom of this.&lt;/p>
&lt;p>I caught this particular case while researching another broken test and noticed that one particular test was hanging for several second before passing every time. After digging in, I found that the view being tested called a method, which called a method, which executed a task that included a &lt;code>requests.post()&lt;/code> call. It was quick to mock out the first method call that spawned it all, but I was worried about next time we test against that view and forget the mock again.&lt;/p>
&lt;p>After a little research I found that Nose provides the ability to run package and module level setup and teardown functions. Simply add &lt;code>setUpPackage&lt;/code>, &lt;code>setUpModule&lt;/code>, &lt;code>tearDownPackage&lt;/code> and &lt;code>tearDownModule&lt;/code> functions to the &lt;code>__init__.py&lt;/code> of the package or module appropriately and you&amp;#39;re free to do any setup and cleanup you need for the package or module as a whole. This can include setting up fixtures, test databases, or, as I discovered, mocking. Placing the following code in the &lt;code>__init__.py&lt;/code> of my project, quickly identified another instance where we were making outbound requests that we weren&amp;#39;t catching.&lt;/p>
&lt;pre class="line-numbers">
&lt;code class="language-python">pkg_mox = None
def setUpPackage():
global pkg_mox
import urllib
import urllib2
import mox
import requests
pkg_mox = mox.Mox()
pkg_mox.StubOutWithMock(requests.api.sessions.Session, &amp;#39;send&amp;#39;)
pkg_mox.StubOutWithMock(urllib.URLopener, &amp;#39;open&amp;#39;)
pkg_mox.StubOutWithMock(urllib2.OpenerDirector, &amp;#39;open&amp;#39;)
pkg_mox.ReplayAll()
def tearDownPackage():
pkg_mox.VerifyAll()
pkg_mox.UnsetStubs()&lt;/code>&lt;/pre>
&lt;p>Cases where requests were being made, but not properly mocked, raised the &amp;quot;Unexpected method called&amp;quot; exception you&amp;#39;d expect, including the traceback to the offending test.&lt;/p>
&lt;p>Through the years, we&amp;#39;ve gone through a few iterations on how we like making our http calls. While we&amp;#39;ve settled on the requests library for now you&amp;#39;re always going to be dealing with legacy code. This flow catches calls to all three major request libraries, just in case. Mocking the underlying methods of the libraries catches all of the common paths for executing requests for each, while still allowing those common cases to be properly mocked in your test code.&lt;/p>
&lt;p>I&amp;#39;m certain there are many other libraries that this could be helpful for. Now let&amp;#39;s get to writing better unit tests.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Decorators vs Mixins for Django Class-Based Views</title><link>https://jsatt.github.io/blog/decorators-vs-mixins-for-django-class-based-views/</link><pubDate>Mon, 13 Jan 2014 20:21:58 +0000</pubDate><guid>https://jsatt.github.io/blog/decorators-vs-mixins-for-django-class-based-views/</guid><description>
&lt;p>I&amp;#39;ve been huge fan of Django&amp;#39;s class-based views (CBVs) since I first tried them out in Django 1.4.&amp;nbsp; While they&amp;#39;re much more complicated then the classic function based views, once you understand how they work, they&amp;#39;re much more powerful, flexible and allow for DRYer code. I highly recommend anyone who hasn&amp;#39;t delved into CBVs take a look at &lt;a href="http://ccbv.co.uk/">Class Class-based&lt;/a>&lt;a href="http://ccbv.co.uk/"> views&lt;/a> or &lt;a href="https://godjango.com/15-class-based-views-part-1-templateview-and-redirectview/">GoDjango&lt;/a>.&lt;/p>
&lt;p>However, one of the early issues I ran into was for views that required Django user permissions. With function based views, Django&amp;#39;s auth application provides decorators to check that users are logged in, have a specific permission, or pass other custom checks the developer can provide. You simply add the decorator to the view...&lt;/p>
&lt;pre>&lt;code class="language-python">@permission_required(&amp;#39;auth.change_user&amp;#39;)
def user_list(request):
...&lt;/code>&lt;/pre>
&lt;p>And now any user that doesn&amp;#39;t have the required permission is redirected to the login page.&lt;/p>
&lt;p>For CBVs though, it&amp;#39;s not quite that simple, it&amp;#39;s the dispatch method that begins the process that you&amp;#39;re used to picking up from with function based views. This means that to apply a decorator to the CBV, you have a couple of options.&lt;/p>
&lt;p>You can override the dispatch method to apply the decorator:&lt;/p>
&lt;pre>&lt;code class="language-python">class UserListView(ListView):
model = User
@my_custom_decorator
def dispatch(self, request, *args, **kwargs):
return super(UserListView, self).dispatch(request, *args, **kwargs)&lt;/code>&lt;/pre>
&lt;p>You could write a reusable class view decorator:&lt;/p>
&lt;pre>&lt;code class="language-python">def class_view_decorator(function_decorator):
def deco(View):
View.dispatch = method_decorator(function_decorator)(View.dispatch)
return View
return deco
@class_view_decorator(my_custom_decorator)
class UserListView(ListView):
model = User&lt;/code>&lt;/pre>
&lt;p>This second option is the one that I&amp;#39;ve lived with for a long time. It&amp;#39;s relatively clean and DRY and not much different than what you&amp;#39;re used to seeing with function views.&lt;/p>
&lt;p>But recently I was pointed toward &lt;a href="https://github.com/brack3t/django-braces">Django Braces&lt;/a>. It&amp;#39;s a library of commonly (and not-so-commonly) needed view decorator-like functionality in the form of mixins. I immediately realized that when thinking about the functionality these decorators provide, I was stuck in the old way of doing things rather than looks at these views from an OOP perspective.&lt;/p>
&lt;p>Writing these decorators as mixins instead, makes them more flexible, extensible and DRYer, just like the move from function views to CBVs. You&amp;#39;re able to build a base mixin that others can inherit from and extend. What would have been arguments to the decorator can are much cleaner as class attributes for the view that&amp;#39;s extending it. You can even have views that apply affect the mixin on a one-off basis. Take for instance the &amp;lt;code&amp;gt;UserCheckMixin&amp;lt;/code&amp;gt; below.&lt;/p>
&lt;pre>&lt;code class="language-python">class UserCheckMixin(object):
user_check_failure_path = &amp;#39;&amp;#39; # can be path, url name or reverse_lazy
def check_user(self, user):
return True
def user_check_failed(self, request, *args, **kwargs):
return redirect(self.user_check_failure_path)
def dispatch(self, request, *args, **kwargs):
if not self.check_user(request.user):
return self.user_check_failed(request, *args, **kwargs)
return super(UserCheckMixin, self).dispatch(request, *args, **kwargs)&lt;/code>&lt;/pre>
&lt;p>I now simply add the mixin to a view:&lt;/p>
&lt;pre>&lt;code class="language-python">class UserListView(UserCheckMixin, ListView):
model = User
user_check_failure_path = &amp;#39;auth_login&amp;#39;
def check_user(self, user)
# do some check against the user here and return True or False&lt;/code>&lt;/pre>
&lt;p>You can extend it to build a more useful mixin:&lt;/p>
&lt;pre>&lt;code class="language-python">class PermissionRequiredMixin(UserCheckMixin):
user_check_failure_path = &amp;#39;auth_login&amp;#39;
permission_required = None
def check_user(self, user):
return user.has_perm(self.permission_required)
class UserListView(PermissionRequredMixin, ListView):
model = User
permission_required = &amp;#39;auth.change_user&amp;#39;
user_check_failure_path = &amp;#39;auth_login&amp;#39;&lt;/code>&lt;/pre>
&lt;p>As you can see, once you&amp;#39;ve defined the &lt;code>PermissionRequiredMixin&lt;/code>, it&amp;#39;s much cleaner, more object-oriented and I would argue more pythonic.&lt;/p>
&lt;p>If you&amp;#39;re relying on built-in decorators, the &lt;code>class_view_decorator&lt;/code> is a pretty helpful tool to have in your codebase. But for anything custom you may be writing, it really is hard to beat a mixin.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator><category>development</category></item><item><title>Fresh start</title><link>https://jsatt.github.io/blog/fresh-start/</link><pubDate>Sat, 11 Jan 2014 22:35:43 +0000</pubDate><guid>https://jsatt.github.io/blog/fresh-start/</guid><description>
&lt;p>This is going to be a fresh attempt to start blogging again on a regular basis. I plan to share development and brewing experiences, open-sourcing in both cases whenever possible.&lt;/p></description><dc:creator>Jeremy Satterfield</dc:creator></item></channel></rss>